{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nONQt2yzChS"
   },
   "source": [
    "# Import necessary dependencies and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Mw6CalghzChT",
    "outputId": "668fb506-16b5-4495-d64b-49a56ff8f3b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/erika/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9aiit_zChW"
   },
   "source": [
    "# Sample corpus of text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "gwma4atgzChX",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8e39d572-1ae4-4e40-fb97-373b9efca6d1"
   },
   "outputs": [],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!']\n",
    "labels = ['weather', 'weather', 'animals', 'animals', 'weather', 'animals']\n",
    "\n",
    "# carga en un DataFrame los documentos y sus categorías\n",
    "\n",
    "df = pd.DataFrame({\"Document\":corpus,\"Category\": labels})\n",
    "\n",
    "corpus_df = df[[\"Document\",\"Category\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVSzcV5YzCha"
   },
   "source": [
    "# Simple text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category\n",
       "0                     The sky is blue and beautiful.  weather\n",
       "1                  Love this blue and beautiful sky!  weather\n",
       "2       The quick brown fox jumps over the lazy dog.  animals\n",
       "3   The brown fox is quick and the blue dog is lazy!  animals\n",
       "4  The sky is very blue and the sky is very beaut...  weather\n",
       "5        The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bsG37EXQzChb"
   },
   "outputs": [],
   "source": [
    "#usa nltk.WordPunctTokenizer()\n",
    "\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words= nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def normalize_document(doc):\n",
    "    doc = re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",doc,re.I)\n",
    "    # ^ que NO sea \\s es espacio en blanco, lo cambia a \"\", re.I es ignore case\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "# usa stop words\n",
    "# normaliza el texto (que solo sean números, espacios en blanco y caracteres de la \"a\" a la \"z\")\n",
    "# asegúrate de que todo el texto esté en minúsculas\n",
    "\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    #print(tokens)\n",
    "# te puede ayudar usar np.vectorize()\n",
    "\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    #print(filtered_tokens)\n",
    "    doc = \" \".join(filtered_tokens)\n",
    "    return doc\n",
    "normalize_corpus=np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sky blue beatiful'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = \"The sky is blue and beatiful\"\n",
    "\n",
    "frase_n=normalize_document(frase)\n",
    "frase_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68xbXgi5PWAJ"
   },
   "source": [
    "Veamos qué hace np.vectorize().\n",
    "\n",
    "Imaginemos una función que acepta un número y devuelve True o False si el número es o no par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyPBNKMRPUN5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SxPKANgWPU2e",
    "outputId": "471d5e4c-f4e8-440a-c252-cd05c679837e"
   },
   "outputs": [],
   "source": [
    "# Probemos la función con un par de valores:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "adg1OLpOPU9n",
    "outputId": "e7adab67-cbf2-41c4-ed03-2ef2e0acba2a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdj7AKixP1qF"
   },
   "source": [
    "Esta función acepta un escalar, por lo que intentar usarla con un array NumPy devolverá un error. Pero podemos \"vectorizarla\" con la función np.vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUyljiHLPvtg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "JVItnnfwPv2V",
    "outputId": "96d65832-f372-4232-d43d-26cc714c77bb"
   },
   "outputs": [],
   "source": [
    "# Ahora ya es posible usarla con arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZzl7AJxQC4e"
   },
   "source": [
    "Volvamos a procesamiento de Texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "TmtgYKiuwiXk",
    "outputId": "b4ebea4c-cc27-4864-8b9d-2446bf44dc08"
   },
   "outputs": [],
   "source": [
    "# muestra el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "Q8H15y2fxMZd",
    "outputId": "15cf992d-be7d-402a-8d90-0a34f8bd37d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sky blue beatiful'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normaliza una frase\n",
    "frase = \"The sky is blue and beatiful\"\n",
    "\n",
    "frase_n=normalize_document(frase)\n",
    "frase_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "TR1SOybCzChd",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6ba55bb3-124d-43be-f2f3-c56568cb4499"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog', 'brown fox quick blue dog lazy',\n",
       "       'sky blue sky beautiful today', 'dog lazy brown fox quick'],\n",
       "      dtype='<U30')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normaliza el corpus\n",
    "norm_corpus=normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "h3Gs4WimybgG",
    "outputId": "08f89898-c535-4a09-bdd6-5fb66693518f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3f00af238239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "\n",
    "vocab = cv.get_feature_names()\n",
    "print(vocab)\n",
    "pd.DataFrame(cv_matrix, columns = vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yVLO7myzChg"
   },
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "tSa-cqPjzChh",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "84b1ff09-232e-4c8c-b9d8-dd9570084883"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# usa countvectorizer con el corpus normalizado\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "\n",
    "# son los cortes para el vocabulario segun las frecuencias en los docuets\n",
    "#aqui estamos cogiendo todas\n",
    "#tanto las palabras que aparecen muy poco 0\n",
    "#como las que aparecen mucho 1 \n",
    "\n",
    "cv_matrix =cv.fit_transform(norm_corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "zP6VHVkTzChk",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a5e8def5-8ab1-4056-e484-342637b396d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# muestra el vocabulario en una matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDKfW8IUzChm"
   },
   "source": [
    "# Bag of N-Grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4jty5YpHzChn",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4cbd06ad-cb88-4fe8-e987-72c7ef9c35f4"
   },
   "outputs": [],
   "source": [
    "bv= CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "bigrams = pd.DataFrame(bv_matrix, columns = vocab)\n",
    "\n",
    "# calcula bigramas (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "MdzUEjJHR33n",
    "outputId": "34afa233-b4a6-41b4-a64e-a1a5ccd6962f"
   },
   "outputs": [],
   "source": [
    "bigrams# muestra los bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFAKSu8hzChp"
   },
   "source": [
    "# TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "qp74wH6AzChq",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "50899291-e870-4752-cf3b-3e7c631ae9c5"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix=tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2),columns = vocab)# usa tf-idf en el corpus normalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7JlMRxkzChs"
   },
   "source": [
    "# Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "Dz8oW5d0zChs",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "71ad6d10-b5f7-4c19-cf7c-84e0db804fcb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# usa la similitud de coseno en el tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv_vH9brzChu"
   },
   "source": [
    "## Clustering documents using similarity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "x7Xd7eMqzChv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6d1d6f3a-2953-4b07-85c4-fd3dea3bba0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# haz un kmeans con 2 clusters\n",
    "# haz un fit_transform de la similitud del coseno anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yweeXV9ZzChx"
   },
   "source": [
    "# Topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "rEvcsvuvzChx",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1c15b3f1-ea4b-4d34-a567-cf4f71ffc18b"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# usa LatentDirichletAllocation y calcula 2 topics del tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fScUGR8YzChz"
   },
   "source": [
    "## Show topics and their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "9DCs6JTpzCh0",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4088257b-325d-41c3-9cdb-b9f323031466"
   },
   "outputs": [],
   "source": [
    "# imprimo el vocabulario perteneciente a cada tópico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up2RJ99dzCh2"
   },
   "source": [
    "## Clustering documents using topic model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "W6w9TOaBzCh2",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "dc3d5887-b24f-419d-fa26-41ceeb4c64ae"
   },
   "outputs": [],
   "source": [
    "# ahora agrupo en clusters usando los tópicos, no la similaridad basada en cosenos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7RRX2LezCh5"
   },
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMTrPk6HzCh6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Ahora voy a trabajar mapear palabras en vectores\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdNSrx2BzSWM"
   },
   "source": [
    "size: The number of dimensions of the embeddings and the default is 100.\n",
    "\n",
    "window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "\n",
    "min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "hIaT477zzCh8",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6d865e05-6537-4058-8cff-373424d6d938"
   },
   "outputs": [],
   "source": [
    "\n",
    "# de palabra a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTg0vKMTzCh-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "   \n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model.wv, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "JY4qgba1zCiA",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b7956d0c-20aa-4b08-ab50-1ebb74774314"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "xbSslODazCiC",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "912273b4-c993-4dfc-8f47-529363acc951"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# en affinitypropagation no se especifica el número de clusters, lo encuentra él\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "COLAB_PROFE_2_Ejercicios_TextData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
